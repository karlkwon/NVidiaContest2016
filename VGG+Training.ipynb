{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread, imresize\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_typeshape(img):\n",
    "    print (\"Type is %s\" % (type(img)))\n",
    "    print (\"Shape is %s\" % (img.shape,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataList(pathName):\n",
    "    data_img_name = []\n",
    "    data_label = []\n",
    "    label_list = []\n",
    "\n",
    "    for file in os.listdir(pathName):\n",
    "        label_list.append(file)\n",
    "\n",
    "    # http://stackoverflow.com/questions/3964681/find-all-files-in-directory-with-extension-txt-in-python\n",
    "    for i, label in enumerate(label_list):\n",
    "\n",
    "#         print('>> ' + str(i) + ':' + label)\n",
    "        for file2 in os.listdir(pathName + os.sep + label):\n",
    "            data_img_name.append(pathName + os.sep + label + os.sep + file2)\n",
    "            data_label.append(i)\n",
    "            \n",
    "    return data_img_name, data_label, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = \"/data/nv_dlcontest_dataset/train_VGG\"\n",
    "\n",
    "data_img_name, data_label, label_list = dataList(root_path)\n",
    "\n",
    "eye_ref = np.eye(len(label_list), len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 512)\n"
     ]
    }
   ],
   "source": [
    "with open('/data/nv_dlcontest_dataset/train_VGG/apple_pie/100605') as f:\n",
    "    weight = np.load(f)\n",
    "    print weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "img_x = 32\n",
    "img_y = 32\n",
    "img_color = 512\n",
    "img_data = img_x*img_y*img_color\n",
    "\n",
    "n_input = img_x*img_y*img_color\n",
    "n_output = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainData(idxList, data_img_name, data_label):\n",
    "    num_data = len(idxList)\n",
    "    _img = np.ndarray(shape=(num_data, img_x, img_y, img_color), dtype=float)\n",
    "    _label = np.ndarray(shape=(num_data, len(label_list)), dtype=int)\n",
    "    \n",
    "    row = 0\n",
    "#     for i, idx in enumerate(idxList):\n",
    "    for idx in idxList:\n",
    "\n",
    "        # file loading\n",
    "        tmp_img = None\n",
    "        tmp_fname = data_img_name[idx]\n",
    "        with open(tmp_fname) as f:\n",
    "            tmp_img = np.load(f)\n",
    "            \n",
    "        if len(tmp_img.shape)<3:\n",
    "            continue\n",
    "        \n",
    "        _img[row] = tmp_img\n",
    "        _label[row] = eye_ref[data_label[idx], :]\n",
    "\n",
    "        row += 1\n",
    "        \n",
    "    return _img[:row, :], _label[:row, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple_pie\t    escargots\t\t     ojingeo_bokkeum\r\n",
      "baby_back_ribs\t    falafel\t\t     omelette\r\n",
      "baklava\t\t    filet_mignon\t     onion_rings\r\n",
      "beef_carpaccio\t    fish_and_chips\t     oysters\r\n",
      "beef_tartare\t    foie_gras\t\t     pad_thai\r\n",
      "beet_salad\t    french_fries\t     paella\r\n",
      "beignets\t    french_onion_soup\t     pajeon\r\n",
      "bibimbap\t    french_toast\t     pancakes\r\n",
      "bossam\t\t    fried_calamari\t     panna_cotta\r\n",
      "bread_pudding\t    fried_rice\t\t     peking_duck\r\n",
      "breakfast_burrito   frozen_yogurt\t     pho\r\n",
      "bruschetta\t    galbijjim\t\t     pizza\r\n",
      "bulgogi\t\t    galchijorim\t\t     pork_chop\r\n",
      "caesar_salad\t    ganjang_gejang\t     poutine\r\n",
      "cannoli\t\t    garlic_bread\t     prime_rib\r\n",
      "caprese_salad\t    gimbob\t\t     pulled_pork_sandwich\r\n",
      "carrot_cake\t    gnocchi\t\t     ramen\r\n",
      "ceviche\t\t    greek_salad\t\t     ravioli\r\n",
      "cheese_plate\t    grilled_cheese_sandwich  red_velvet_cake\r\n",
      "cheesecake\t    grilled_salmon\t     risotto\r\n",
      "chicken_curry\t    guacamole\t\t     samgupsal\r\n",
      "chicken_quesadilla  gyoza\t\t     samgyetang\r\n",
      "chicken_wings\t    hamburger\t\t     samosa\r\n",
      "chocolate_cake\t    hot_and_sour_soup\t     sashimi\r\n",
      "chocolate_mousse    hot_dog\t\t     scallops\r\n",
      "churros\t\t    huevos_rancheros\t     seaweed_salad\r\n",
      "clam_chowder\t    hummus\t\t     shrimp_and_grits\r\n",
      "club_sandwich\t    ice_cream\t\t     soondae\r\n",
      "crab_cakes\t    jeyuk_bokkeum\t     soondubu_jjigae\r\n",
      "creme_brulee\t    jjajangmyeon\t     spaghetti_bolognese\r\n",
      "croque_madame\t    kimchi\t\t     spaghetti_carbonara\r\n",
      "cup_cakes\t    lasagna\t\t     spring_rolls\r\n",
      "daegaejjim\t    lobster_bisque\t     steak\r\n",
      "dakbokkeumtang\t    lobster_roll_sandwich    strawberry_shortcake\r\n",
      "deviled_eggs\t    macaroni_and_cheese      sushi\r\n",
      "doenjang_chigae     macarons\t\t     tacos\r\n",
      "donuts\t\t    miso_soup\t\t     takoyaki\r\n",
      "dumplings\t    mussels\t\t     tiramisu\r\n",
      "edamame\t\t    nachos\t\t     tuna_tartare\r\n",
      "eggs_benedict\t    nangmyeon\t\t     waffles\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/nv_dlcontest_dataset/train_VGG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "weights = {\n",
    "    'wd1': tf.Variable(tf.random_normal([img_data, n_output], stddev=0.1)),\n",
    "}\n",
    "biases = {\n",
    "    'bd1': tf.Variable(tf.random_normal([n_output], stddev=0.1)),\n",
    "}\n",
    "\n",
    "def conv_basic(_input, _w, _b, _keepratio):\n",
    "    # Input\n",
    "    _input_r = tf.reshape(_input, shape=[-1, img_y * img_x * img_color])\n",
    "    \n",
    "    # Fc1\n",
    "    _out = tf.add(tf.matmul(_input_r, _w['wd1']), _b['bd1'])\n",
    "    \n",
    "    # Return everythin\n",
    "    out = {\n",
    "        'input_r': _input_r,\n",
    "        'out': _out\n",
    "    }\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# weights = {\n",
    "#     'wd1': tf.Variable(tf.random_normal([img_data, 2048], stddev=0.1)),\n",
    "#     'wd2': tf.Variable(tf.random_normal([2048, n_output], stddev=0.1)),\n",
    "# }\n",
    "# biases = {\n",
    "#     'bd1': tf.Variable(tf.random_normal([2048], stddev=0.1)),\n",
    "#     'bd2': tf.Variable(tf.random_normal([n_output], stddev=0.1)),\n",
    "# }\n",
    "\n",
    "# def conv_basic(_input, _w, _b, _keepratio):\n",
    "#     # Input\n",
    "#     _input_r = tf.reshape(_input, shape=[-1, img_y * img_x * img_color])\n",
    "    \n",
    "#     # Fc1\n",
    "#     _fc1 = tf.nn.relu(tf.add(tf.matmul(_input_r, _w['wd1']), _b['bd1']))\n",
    "#     _fc_dr1 = tf.nn.dropout(_fc1, _keepratio)\n",
    "    \n",
    "#     # Fc2\n",
    "#     _out = tf.add(tf.matmul(_fc_dr1, _w['wd2']), _b['bd2'])\n",
    "    \n",
    "#     # Return everythin\n",
    "#     out = {\n",
    "#         'input_r': _input_r,\n",
    "#         'fc1': _fc1,\n",
    "#         'fc_dr1': _fc_dr1,\n",
    "#         'out': _out\n",
    "#     }\n",
    "    \n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, img_y, img_x, img_color])\n",
    "y = tf.placeholder(tf.float32, [None, n_output])\n",
    "keepratio = tf.placeholder(tf.float32)\n",
    "\n",
    "_pred = conv_basic(x, weights, biases, keepratio)['out']\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(_pred, y))\n",
    "optm = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "_corr = tf.equal(tf.argmax(_pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(_corr, tf.float32))\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Saver\n",
    "save_step = 1\n",
    "savedir = 'nets_VGG_test/'\n",
    "saver = tf.train.Saver(max_to_keep=training_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir nets_VGG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "do_train = 1\n",
    "\n",
    "# Do some optimizations\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Batch turn: 0, avg_cost: 3.82374969482\n",
      ">>> Batch turn: 10, avg_cost: 64.5908514404\n",
      ">>> Batch turn: 20, avg_cost: 122.860076904\n",
      ">>> Batch turn: 30, avg_cost: 173.809180298\n",
      ">>> Batch turn: 40, avg_cost: 218.965236816\n",
      ">>> Batch turn: 50, avg_cost: 258.355927124\n",
      ">>> Batch turn: 60, avg_cost: 294.726842957\n",
      ">>> Batch turn: 70, avg_cost: 327.905557251\n",
      ">>> Batch turn: 80, avg_cost: 361.843382874\n",
      ">>> Batch turn: 90, avg_cost: 394.056391907\n",
      ">>> Batch turn: 100, avg_cost: 422.695877228\n",
      ">>> Batch turn: 110, avg_cost: 452.562248993\n",
      ">>> Batch turn: 120, avg_cost: 482.600773163\n",
      ">>> Batch turn: 130, avg_cost: 510.648816681\n",
      ">>> Batch turn: 140, avg_cost: 536.65723053\n",
      ">>> Batch turn: 150, avg_cost: 563.639736786\n",
      ">>> Batch turn: 160, avg_cost: 588.800364532\n",
      ">>> Batch turn: 170, avg_cost: 612.067735443\n",
      ">>> Batch turn: 180, avg_cost: 635.588895416\n",
      ">>> Batch turn: 190, avg_cost: 659.564342041\n",
      ">>> Batch turn: 200, avg_cost: 681.742223511\n",
      ">>> Batch turn: 210, avg_cost: 706.539822083\n",
      ">>> Batch turn: 220, avg_cost: 727.262167511\n",
      ">>> Batch turn: 230, avg_cost: 750.509951935\n",
      ">>> Batch turn: 240, avg_cost: 774.212413177\n",
      ">>> Batch turn: 250, avg_cost: 794.171265106\n",
      ">>> Batch turn: 260, avg_cost: 816.052918091\n",
      ">>> Batch turn: 270, avg_cost: 834.224573975\n",
      ">>> Batch turn: 280, avg_cost: 853.353748779\n",
      ">>> Batch turn: 290, avg_cost: 872.375863037\n",
      ">>> Batch turn: 300, avg_cost: 890.999101868\n",
      ">>> Batch turn: 310, avg_cost: 909.365611877\n",
      ">>> Batch turn: 320, avg_cost: 928.862654877\n",
      ">>> Batch turn: 330, avg_cost: 946.155563354\n",
      ">>> Batch turn: 340, avg_cost: 964.234483032\n",
      ">>> Batch turn: 350, avg_cost: 982.694792328\n",
      ">>> Batch turn: 360, avg_cost: 1001.1886972\n",
      ">>> Batch turn: 370, avg_cost: 1017.91019409\n",
      ">>> Batch turn: 380, avg_cost: 1034.94495636\n",
      ">>> Batch turn: 390, avg_cost: 1051.30980194\n",
      ">>> Batch turn: 400, avg_cost: 1066.3934346\n",
      ">>> Batch turn: 410, avg_cost: 1084.03034584\n",
      ">>> Batch turn: 420, avg_cost: 1100.47162025\n",
      ">>> Batch turn: 430, avg_cost: 1117.08978371\n",
      ">>> Batch turn: 440, avg_cost: 1134.32050941\n",
      ">>> Batch turn: 450, avg_cost: 1153.14384529\n",
      ">>> Batch turn: 460, avg_cost: 1169.69540413\n",
      ">>> Batch turn: 470, avg_cost: 1185.15484329\n",
      ">>> Batch turn: 480, avg_cost: 1200.63512993\n",
      ">>> Batch turn: 490, avg_cost: 1216.30154968\n",
      ">>> Batch turn: 500, avg_cost: 1231.28133575\n",
      ">>> Batch turn: 510, avg_cost: 1248.54659454\n",
      ">>> Batch turn: 520, avg_cost: 1263.82190247\n",
      ">>> Batch turn: 530, avg_cost: 1278.04166443\n",
      ">>> Batch turn: 540, avg_cost: 1291.95314819\n",
      ">>> Batch turn: 550, avg_cost: 1307.74377846\n",
      ">>> Batch turn: 560, avg_cost: 1322.55222527\n",
      ">>> Batch turn: 570, avg_cost: 1338.16388313\n",
      ">>> Batch turn: 580, avg_cost: 1351.62866402\n",
      ">>> Batch turn: 590, avg_cost: 1363.77854782\n",
      ">>> Batch turn: 600, avg_cost: 1376.89765305\n",
      ">>> Batch turn: 610, avg_cost: 1391.85416\n",
      ">>> Batch turn: 620, avg_cost: 1404.80367691\n",
      ">>> Batch turn: 630, avg_cost: 1417.90389389\n",
      ">>> Batch turn: 640, avg_cost: 1432.2650473\n",
      ">>> Batch turn: 650, avg_cost: 1445.70545494\n",
      ">>> Batch turn: 660, avg_cost: 1458.11633217\n",
      ">>> Batch turn: 670, avg_cost: 1471.15429276\n",
      ">>> Batch turn: 680, avg_cost: 1482.92055847\n",
      ">>> Batch turn: 690, avg_cost: 1496.29864437\n",
      ">>> Batch turn: 700, avg_cost: 1509.16967083\n",
      ">>> Batch turn: 710, avg_cost: 1521.22442402\n",
      ">>> Batch turn: 720, avg_cost: 1534.46585461\n",
      ">>> Batch turn: 730, avg_cost: 1544.24136738\n",
      ">>> Batch turn: 740, avg_cost: 1556.15128788\n",
      ">>> Batch turn: 750, avg_cost: 1567.86712585\n",
      ">>> Batch turn: 760, avg_cost: 1579.22759476\n",
      ">>> Batch turn: 770, avg_cost: 1591.28824051\n",
      ">>> Batch turn: 780, avg_cost: 1603.72967407\n",
      ">>> Batch turn: 790, avg_cost: 1615.81590668\n",
      ">>> Batch turn: 800, avg_cost: 1626.17620235\n",
      ">>> Batch turn: 810, avg_cost: 1638.2269762\n",
      ">>> Batch turn: 820, avg_cost: 1648.66744221\n",
      ">>> Batch turn: 830, avg_cost: 1659.24676193\n",
      ">>> Batch turn: 840, avg_cost: 1670.12305534\n",
      ">>> Batch turn: 850, avg_cost: 1680.26630066\n",
      ">>> Batch turn: 860, avg_cost: 1689.77656761\n",
      ">>> Batch turn: 870, avg_cost: 1699.80187233\n",
      ">>> Batch turn: 880, avg_cost: 1711.33003769\n",
      ">>> Batch turn: 890, avg_cost: 1722.71008812\n",
      ">>> Batch turn: 900, avg_cost: 1732.67089737\n",
      ">>> Batch turn: 910, avg_cost: 1742.47232117\n",
      ">>> Batch turn: 920, avg_cost: 1750.72322552\n",
      ">>> Batch turn: 930, avg_cost: 1760.16658108\n",
      ">>> Batch turn: 940, avg_cost: 1769.29931866\n",
      ">>> Batch turn: 950, avg_cost: 1779.9406908\n",
      ">>> Batch turn: 960, avg_cost: 1789.570648\n",
      ">>> Batch turn: 970, avg_cost: 1798.94234898\n",
      ">>> Batch turn: 980, avg_cost: 1808.59869045\n",
      ">>> Batch turn: 990, avg_cost: 1816.91005928\n",
      ">>> Batch turn: 1000, avg_cost: 1826.13667511\n",
      ">>> Batch turn: 1010, avg_cost: 1834.40641701\n",
      ">>> Batch turn: 1020, avg_cost: 1843.90943245\n",
      ">>> Batch turn: 1030, avg_cost: 1853.59388649\n",
      "Epoch: 000/003 cost: 1861.034280567\n",
      "Training accuracy: 0.760\n",
      ">>> Batch turn: 0, avg_cost: 0.587005081177\n",
      ">>> Batch turn: 10, avg_cost: 9.85859939575\n",
      ">>> Batch turn: 20, avg_cost: 18.650653038\n",
      ">>> Batch turn: 30, avg_cost: 26.6333509445\n",
      ">>> Batch turn: 40, avg_cost: 35.8152569199\n",
      ">>> Batch turn: 50, avg_cost: 44.1492161942\n",
      ">>> Batch turn: 60, avg_cost: 52.0156615448\n",
      ">>> Batch turn: 70, avg_cost: 59.7624620438\n",
      ">>> Batch turn: 80, avg_cost: 66.9182151794\n",
      ">>> Batch turn: 90, avg_cost: 74.8561255646\n",
      ">>> Batch turn: 100, avg_cost: 83.8557379532\n",
      ">>> Batch turn: 110, avg_cost: 92.8865641785\n",
      ">>> Batch turn: 120, avg_cost: 99.4402822495\n",
      ">>> Batch turn: 130, avg_cost: 106.453675346\n",
      ">>> Batch turn: 140, avg_cost: 114.27242485\n",
      ">>> Batch turn: 150, avg_cost: 122.676124725\n",
      ">>> Batch turn: 160, avg_cost: 129.894360619\n",
      ">>> Batch turn: 170, avg_cost: 137.068477783\n",
      ">>> Batch turn: 180, avg_cost: 144.149955959\n",
      ">>> Batch turn: 190, avg_cost: 152.130102825\n",
      ">>> Batch turn: 200, avg_cost: 159.900133057\n",
      ">>> Batch turn: 210, avg_cost: 166.815650406\n",
      ">>> Batch turn: 220, avg_cost: 173.735772667\n",
      ">>> Batch turn: 230, avg_cost: 180.199151497\n",
      ">>> Batch turn: 240, avg_cost: 187.539402733\n",
      ">>> Batch turn: 250, avg_cost: 195.467755852\n",
      ">>> Batch turn: 260, avg_cost: 201.760169544\n",
      ">>> Batch turn: 270, avg_cost: 206.953215351\n",
      ">>> Batch turn: 280, avg_cost: 215.419635525\n",
      ">>> Batch turn: 290, avg_cost: 222.644740314\n",
      ">>> Batch turn: 300, avg_cost: 230.66833128\n",
      ">>> Batch turn: 310, avg_cost: 237.19466917\n",
      ">>> Batch turn: 320, avg_cost: 244.890856647\n",
      ">>> Batch turn: 330, avg_cost: 251.69028162\n",
      ">>> Batch turn: 340, avg_cost: 259.626609173\n",
      ">>> Batch turn: 350, avg_cost: 266.274469738\n",
      ">>> Batch turn: 360, avg_cost: 272.814053593\n",
      ">>> Batch turn: 370, avg_cost: 278.943623066\n",
      ">>> Batch turn: 380, avg_cost: 284.479952374\n",
      ">>> Batch turn: 390, avg_cost: 288.444440517\n",
      ">>> Batch turn: 400, avg_cost: 294.263788204\n",
      ">>> Batch turn: 410, avg_cost: 298.773767653\n",
      ">>> Batch turn: 420, avg_cost: 306.454064894\n",
      ">>> Batch turn: 430, avg_cost: 312.977626638\n",
      ">>> Batch turn: 440, avg_cost: 319.481524363\n",
      ">>> Batch turn: 450, avg_cost: 325.886725473\n",
      ">>> Batch turn: 460, avg_cost: 331.952156115\n",
      ">>> Batch turn: 470, avg_cost: 337.417707224\n",
      ">>> Batch turn: 480, avg_cost: 342.608037653\n",
      ">>> Batch turn: 490, avg_cost: 347.669886446\n",
      ">>> Batch turn: 500, avg_cost: 352.836882687\n",
      ">>> Batch turn: 510, avg_cost: 357.446444092\n",
      ">>> Batch turn: 520, avg_cost: 362.249133806\n",
      ">>> Batch turn: 530, avg_cost: 367.855296068\n",
      ">>> Batch turn: 540, avg_cost: 373.028077145\n",
      ">>> Batch turn: 550, avg_cost: 379.343030586\n",
      ">>> Batch turn: 560, avg_cost: 383.829741173\n",
      ">>> Batch turn: 570, avg_cost: 389.017427902\n",
      ">>> Batch turn: 580, avg_cost: 393.885350266\n",
      ">>> Batch turn: 590, avg_cost: 398.512724628\n",
      ">>> Batch turn: 600, avg_cost: 404.314857788\n",
      ">>> Batch turn: 610, avg_cost: 408.9564394\n",
      ">>> Batch turn: 620, avg_cost: 413.969592991\n",
      ">>> Batch turn: 630, avg_cost: 419.142159214\n",
      ">>> Batch turn: 640, avg_cost: 423.973528843\n",
      ">>> Batch turn: 650, avg_cost: 428.050644779\n",
      ">>> Batch turn: 660, avg_cost: 432.990867405\n",
      ">>> Batch turn: 670, avg_cost: 438.125144768\n",
      ">>> Batch turn: 680, avg_cost: 443.592361145\n",
      ">>> Batch turn: 690, avg_cost: 449.745727501\n",
      ">>> Batch turn: 700, avg_cost: 453.813806477\n",
      ">>> Batch turn: 710, avg_cost: 457.245150146\n",
      ">>> Batch turn: 720, avg_cost: 460.761012383\n",
      ">>> Batch turn: 730, avg_cost: 465.042703209\n",
      ">>> Batch turn: 740, avg_cost: 468.865576649\n",
      ">>> Batch turn: 750, avg_cost: 472.438469725\n",
      ">>> Batch turn: 760, avg_cost: 477.52064599\n",
      ">>> Batch turn: 770, avg_cost: 481.230134325\n",
      ">>> Batch turn: 780, avg_cost: 486.197156897\n",
      ">>> Batch turn: 790, avg_cost: 490.730846777\n",
      ">>> Batch turn: 800, avg_cost: 495.106976614\n",
      ">>> Batch turn: 810, avg_cost: 498.976858044\n",
      ">>> Batch turn: 820, avg_cost: 503.156953526\n",
      ">>> Batch turn: 830, avg_cost: 508.132123508\n",
      ">>> Batch turn: 840, avg_cost: 512.586617146\n",
      ">>> Batch turn: 850, avg_cost: 516.401263547\n",
      ">>> Batch turn: 860, avg_cost: 520.16977921\n",
      ">>> Batch turn: 870, avg_cost: 523.368918595\n",
      ">>> Batch turn: 880, avg_cost: 527.86745852\n",
      ">>> Batch turn: 890, avg_cost: 532.13762671\n",
      ">>> Batch turn: 900, avg_cost: 535.769281487\n",
      ">>> Batch turn: 910, avg_cost: 541.533035913\n",
      ">>> Batch turn: 920, avg_cost: 544.983240628\n",
      ">>> Batch turn: 930, avg_cost: 548.271348338\n",
      ">>> Batch turn: 940, avg_cost: 551.792392688\n",
      ">>> Batch turn: 950, avg_cost: 554.827157712\n",
      ">>> Batch turn: 960, avg_cost: 559.236907926\n",
      ">>> Batch turn: 970, avg_cost: 562.150400128\n",
      ">>> Batch turn: 980, avg_cost: 566.22229363\n",
      ">>> Batch turn: 990, avg_cost: 570.434047418\n",
      ">>> Batch turn: 1000, avg_cost: 573.027459903\n",
      ">>> Batch turn: 1010, avg_cost: 575.768730397\n",
      ">>> Batch turn: 1020, avg_cost: 579.022892118\n",
      ">>> Batch turn: 1030, avg_cost: 581.835478415\n",
      "Epoch: 001/003 cost: 584.932376628\n",
      "Training accuracy: 0.950\n",
      ">>> Batch turn: 0, avg_cost: 0.274309005737\n",
      ">>> Batch turn: 10, avg_cost: 3.26805978298\n",
      ">>> Batch turn: 20, avg_cost: 5.646181674\n",
      ">>> Batch turn: 30, avg_cost: 8.26625842571\n",
      ">>> Batch turn: 40, avg_cost: 12.3570707405\n",
      ">>> Batch turn: 50, avg_cost: 16.4148531806\n",
      ">>> Batch turn: 60, avg_cost: 19.2051208007\n",
      ">>> Batch turn: 70, avg_cost: 23.1957203567\n",
      ">>> Batch turn: 80, avg_cost: 26.456703614\n",
      ">>> Batch turn: 90, avg_cost: 30.5088816345\n",
      ">>> Batch turn: 100, avg_cost: 33.9519444835\n",
      ">>> Batch turn: 110, avg_cost: 36.35876109\n",
      ">>> Batch turn: 120, avg_cost: 39.2537061584\n",
      ">>> Batch turn: 130, avg_cost: 42.4937553251\n",
      ">>> Batch turn: 140, avg_cost: 45.4540780866\n",
      ">>> Batch turn: 150, avg_cost: 48.0147529447\n",
      ">>> Batch turn: 160, avg_cost: 50.8013833845\n",
      ">>> Batch turn: 170, avg_cost: 53.2930310571\n",
      ">>> Batch turn: 180, avg_cost: 55.1131279427\n",
      ">>> Batch turn: 190, avg_cost: 57.6913995796\n",
      ">>> Batch turn: 200, avg_cost: 60.0180280739\n",
      ">>> Batch turn: 210, avg_cost: 62.2064943486\n",
      ">>> Batch turn: 220, avg_cost: 65.6484835511\n",
      ">>> Batch turn: 230, avg_cost: 68.6795897371\n",
      ">>> Batch turn: 240, avg_cost: 72.3559911806\n",
      ">>> Batch turn: 250, avg_cost: 74.7032965786\n",
      ">>> Batch turn: 260, avg_cost: 77.8567992711\n",
      ">>> Batch turn: 270, avg_cost: 81.5881078077\n",
      ">>> Batch turn: 280, avg_cost: 84.3942661825\n",
      ">>> Batch turn: 290, avg_cost: 87.2847673193\n",
      ">>> Batch turn: 300, avg_cost: 90.3025805632\n",
      ">>> Batch turn: 310, avg_cost: 92.8457052389\n",
      ">>> Batch turn: 320, avg_cost: 95.3164949003\n",
      ">>> Batch turn: 330, avg_cost: 98.0124947325\n",
      ">>> Batch turn: 340, avg_cost: 100.346879814\n",
      ">>> Batch turn: 350, avg_cost: 101.953681982\n",
      ">>> Batch turn: 360, avg_cost: 104.91342674\n",
      ">>> Batch turn: 370, avg_cost: 107.581786592\n",
      ">>> Batch turn: 380, avg_cost: 109.3103706\n",
      ">>> Batch turn: 390, avg_cost: 111.108981793\n",
      ">>> Batch turn: 400, avg_cost: 114.357461019\n",
      ">>> Batch turn: 410, avg_cost: 116.746432929\n",
      ">>> Batch turn: 420, avg_cost: 118.934100652\n",
      ">>> Batch turn: 430, avg_cost: 121.17526515\n",
      ">>> Batch turn: 440, avg_cost: 123.139517393\n",
      ">>> Batch turn: 450, avg_cost: 124.891747539\n",
      ">>> Batch turn: 460, avg_cost: 126.649303978\n",
      ">>> Batch turn: 470, avg_cost: 129.160550573\n",
      ">>> Batch turn: 480, avg_cost: 131.724835908\n",
      ">>> Batch turn: 490, avg_cost: 134.399656706\n",
      ">>> Batch turn: 500, avg_cost: 136.948937691\n",
      ">>> Batch turn: 510, avg_cost: 139.1645682\n",
      ">>> Batch turn: 520, avg_cost: 141.01078691\n",
      ">>> Batch turn: 530, avg_cost: 142.957987218\n",
      ">>> Batch turn: 540, avg_cost: 145.147214475\n",
      ">>> Batch turn: 550, avg_cost: 147.047724538\n",
      ">>> Batch turn: 560, avg_cost: 148.221152289\n",
      ">>> Batch turn: 570, avg_cost: 150.089145243\n",
      ">>> Batch turn: 580, avg_cost: 152.46489528\n",
      ">>> Batch turn: 590, avg_cost: 154.161455808\n",
      ">>> Batch turn: 600, avg_cost: 155.864667897\n",
      ">>> Batch turn: 610, avg_cost: 157.695560307\n",
      ">>> Batch turn: 620, avg_cost: 159.541995139\n",
      ">>> Batch turn: 630, avg_cost: 161.279392523\n",
      ">>> Batch turn: 640, avg_cost: 163.560005214\n",
      ">>> Batch turn: 650, avg_cost: 164.873970427\n",
      ">>> Batch turn: 660, avg_cost: 166.749688696\n",
      ">>> Batch turn: 670, avg_cost: 168.386308438\n",
      ">>> Batch turn: 680, avg_cost: 169.394042651\n",
      ">>> Batch turn: 690, avg_cost: 171.595726678\n",
      ">>> Batch turn: 700, avg_cost: 173.455756747\n",
      ">>> Batch turn: 710, avg_cost: 175.208192858\n",
      ">>> Batch turn: 720, avg_cost: 177.509364557\n",
      ">>> Batch turn: 730, avg_cost: 179.826023311\n",
      ">>> Batch turn: 740, avg_cost: 182.563778963\n",
      ">>> Batch turn: 750, avg_cost: 184.581863529\n",
      ">>> Batch turn: 760, avg_cost: 185.994071009\n",
      ">>> Batch turn: 770, avg_cost: 188.394754333\n",
      ">>> Batch turn: 780, avg_cost: 190.028073105\n",
      ">>> Batch turn: 790, avg_cost: 192.116511812\n",
      ">>> Batch turn: 800, avg_cost: 193.675329018\n",
      ">>> Batch turn: 810, avg_cost: 194.890108159\n",
      ">>> Batch turn: 820, avg_cost: 195.940544705\n",
      ">>> Batch turn: 830, avg_cost: 197.350320031\n",
      ">>> Batch turn: 840, avg_cost: 199.061872068\n",
      ">>> Batch turn: 850, avg_cost: 200.158979514\n",
      ">>> Batch turn: 860, avg_cost: 202.018244813\n",
      ">>> Batch turn: 870, avg_cost: 203.462505953\n",
      ">>> Batch turn: 880, avg_cost: 205.171737294\n",
      ">>> Batch turn: 890, avg_cost: 206.720559411\n",
      ">>> Batch turn: 900, avg_cost: 208.172087331\n",
      ">>> Batch turn: 910, avg_cost: 209.370517824\n",
      ">>> Batch turn: 920, avg_cost: 210.473304682\n",
      ">>> Batch turn: 930, avg_cost: 212.025422422\n",
      ">>> Batch turn: 940, avg_cost: 213.791859815\n",
      ">>> Batch turn: 950, avg_cost: 215.895006077\n",
      ">>> Batch turn: 960, avg_cost: 217.426414604\n",
      ">>> Batch turn: 970, avg_cost: 218.926318554\n",
      ">>> Batch turn: 980, avg_cost: 220.418084501\n",
      ">>> Batch turn: 990, avg_cost: 221.83632759\n",
      ">>> Batch turn: 1000, avg_cost: 223.226131133\n",
      ">>> Batch turn: 1010, avg_cost: 224.342269668\n",
      ">>> Batch turn: 1020, avg_cost: 225.492068879\n",
      ">>> Batch turn: 1030, avg_cost: 226.158548048\n",
      "Epoch: 002/003 cost: 227.153497639\n",
      "Training accuracy: 0.970\n",
      "Optimization Finished.\n"
     ]
    }
   ],
   "source": [
    "if do_train == 1:\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(data_img_name)/batch_size)\n",
    "        \n",
    "        # Loop over all batchs\n",
    "        for i in range(total_batch):\n",
    "            randidx = np.random.randint(len(data_img_name), size=batch_size)\n",
    "            \n",
    "            batch_xs, batch_ys = getTrainData(randidx, data_img_name, data_label)\n",
    "#             batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            \n",
    "            # Fit training using batch data\n",
    "            sess.run(optm, feed_dict={x: batch_xs, y: batch_ys, keepratio: .7})\n",
    "            \n",
    "            # Compute average loss\n",
    "            avg_cost += sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keepratio: 1.})/batch_xs.shape[0]\n",
    "            \n",
    "            if i%10 == 0:\n",
    "                print (\">>> Batch turn: \" + str(i) + \", avg_cost: \" + str(avg_cost))\n",
    "            \n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch: %03d/%03d cost: %.9f\" % (epoch, training_epochs, avg_cost))\n",
    "            train_acc = sess.run(accr, feed_dict={x: batch_xs, y: batch_ys, keepratio: 1.})\n",
    "            print (\"Training accuracy: %.3f\" % (train_acc))\n",
    "\n",
    "        # Save Net\n",
    "        if epoch % save_step == 0:\n",
    "            saver.save(sess, savedir + 'cnn_1_fcn.ckpt-' + str(epoch))\n",
    "    \n",
    "    print ('Optimization Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataTestList(pathName):\n",
    "    data_img_name = []\n",
    "\n",
    "    for file2 in os.listdir(pathName):\n",
    "        data_img_name.append(pathName + os.sep + file2)\n",
    "            \n",
    "    return data_img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_root_path = \"/data/nv_dlcontest_dataset/test_VGG\"\n",
    "\n",
    "test_data_img_name = dataTestList(test_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTestData(idxList, data_img_name):\n",
    "    num_data = len(idxList)\n",
    "    _img = np.ndarray(shape=(num_data, img_x, img_y, img_color), dtype=float)\n",
    "    \n",
    "    row = 0\n",
    "    for idx in idxList:\n",
    "\n",
    "        # file loading\n",
    "        tmp_img = None\n",
    "        tmp_fname = data_img_name[idx]\n",
    "        with open(tmp_fname) as f:\n",
    "            tmp_img = np.load(f)\n",
    "            \n",
    "        if len(tmp_img.shape)<3:\n",
    "            continue\n",
    "        \n",
    "        _img[row] = tmp_img\n",
    "\n",
    "        row += 1\n",
    "        \n",
    "    return _img[:row, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 100\n",
      "100 : 200\n",
      "200 : 300\n",
      "300 : 400\n",
      "400 : 500\n",
      "500 : 600\n",
      "600 : 700\n",
      "700 : 800\n",
      "800 : 900\n",
      "900 : 1000\n",
      "1000 : 1100\n",
      "1100 : 1200\n",
      "1200 : 1300\n",
      "1300 : 1400\n",
      "1400 : 1500\n",
      "1500 : 1600\n",
      "1600 : 1700\n",
      "1700 : 1800\n",
      "1800 : 1900\n",
      "1900 : 2000\n",
      "2000 : 2100\n",
      "2100 : 2200\n",
      "2200 : 2300\n",
      "2300 : 2400\n",
      "2400 : 2500\n",
      "2500 : 2600\n",
      "2600 : 2700\n",
      "2700 : 2800\n",
      "2800 : 2900\n",
      "2900 : 3000\n",
      "3000 : 3100\n",
      "3100 : 3200\n",
      "3200 : 3300\n",
      "3300 : 3400\n",
      "3400 : 3500\n",
      "3500 : 3600\n",
      "3600 : 3700\n",
      "3700 : 3800\n",
      "3800 : 3900\n",
      "3900 : 4000\n",
      "4000 : 4100\n",
      "4100 : 4200\n",
      "4200 : 4300\n",
      "4300 : 4400\n",
      "4400 : 4500\n",
      "4500 : 4600\n",
      "4600 : 4700\n",
      "4700 : 4800\n",
      "4800 : 4900\n",
      "4900 : 5000\n",
      "5000 : 5100\n",
      "5100 : 5200\n",
      "5200 : 5300\n",
      "5300 : 5400\n",
      "5400 : 5500\n",
      "5500 : 5600\n",
      "5600 : 5700\n",
      "5700 : 5800\n",
      "5800 : 5900\n",
      "5900 : 6000\n",
      "6000 : 6100\n",
      "6100 : 6200\n",
      "6200 : 6300\n",
      "6300 : 6400\n",
      "6400 : 6500\n",
      "6500 : 6600\n",
      "6600 : 6700\n",
      "6700 : 6800\n",
      "6800 : 6900\n",
      "6900 : 7000\n",
      "7000 : 7100\n",
      "7100 : 7200\n",
      "7200 : 7300\n",
      "7300 : 7400\n",
      "7400 : 7500\n",
      "7500 : 7600\n",
      "7600 : 7700\n",
      "7700 : 7800\n",
      "7800 : 7900\n",
      "7900 : 8000\n",
      "8000 : 8100\n",
      "8100 : 8200\n",
      "8200 : 8300\n",
      "8300 : 8400\n",
      "8400 : 8500\n",
      "8500 : 8600\n",
      "8600 : 8700\n",
      "8700 : 8800\n",
      "8800 : 8900\n",
      "8900 : 9000\n",
      "9000 : 9100\n",
      "9100 : 9200\n",
      "9200 : 9300\n",
      "9300 : 9400\n",
      "9400 : 9500\n",
      "9500 : 9600\n",
      "9600 : 9700\n",
      "9700 : 9800\n",
      "9800 : 9900\n",
      "9900 : 10000\n",
      "10000 : 10100\n",
      "10100 : 10200\n",
      "10200 : 10300\n",
      "10300 : 10400\n",
      "10400 : 10500\n",
      "10500 : 10600\n",
      "10600 : 10700\n",
      "10700 : 10800\n",
      "10800 : 10900\n",
      "10900 : 11000\n",
      "11000 : 11100\n",
      "11100 : 11200\n",
      "11200 : 11300\n",
      "11300 : 11400\n",
      "11400 : 11500\n",
      "11500 : 11565\n",
      "(65, 32, 32, 512)\n",
      "(?, 120)\n",
      "[103  46  90  75  44 117 105 115  39 105  47  80 104  96  23  83  14 103\n",
      "   2  15  58   1  58  98  76  68 112  86  75  27 102  52  82  79  61 104\n",
      " 101   7  50  26  82 104  21 105  73  81 117  18  39   7  75  54 105  46\n",
      "  76  78  69  83   0  52 102  18 106  76  40]\n"
     ]
    }
   ],
   "source": [
    "ret = []\n",
    "data_len = len(test_data_img_name)\n",
    "\n",
    "for s, e in zip(range(0, data_len - batch_size, batch_size), range(batch_size, data_len, batch_size)):\n",
    "    print (str(s) + ' : ' + str(e))\n",
    "\n",
    "    batch_xs = getTestData(range(s, e), test_data_img_name)\n",
    "    tmp_pred = sess.run(tf.argmax(_pred, 1), feed_dict={x: batch_xs, keepratio: 1.})\n",
    "    ret.extend(tmp_pred)\n",
    "\n",
    "if data_len/batch_size != 0:\n",
    "    s = data_len - data_len%batch_size\n",
    "    e = data_len\n",
    "    print (str(s) + ' : ' + str(e))\n",
    "\n",
    "    batch_xs = getTestData(range(s, e), test_data_img_name)\n",
    "    tmp_pred = sess.run(tf.argmax(_pred, 1), feed_dict={x: batch_xs, keepratio: 1.})\n",
    "    ret.extend(tmp_pred)\n",
    "    \n",
    "    print (batch_xs.shape)\n",
    "    print (_pred.get_shape())\n",
    "    print (tmp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('mrthinks_0909.csv', 'w') as f:\n",
    "    for i, _id in enumerate(ret):\n",
    "        f.write(test_data_img_name[i].split('/')[-1])\n",
    "        f.write(',')\n",
    "        f.write(label_list[_id])\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98491,spaghetti_carbonara\r\n",
      "602016,oysters\r\n",
      "294445,pad_thai\r\n",
      "368014,hot_dog\r\n",
      "340666,spaghetti_carbonara\r\n",
      "98367,dakbokkeumtang\r\n",
      "64060,oysters\r\n",
      "767141,pancakes\r\n",
      "353444,filet_mignon\r\n",
      "465222,macarons\r\n"
     ]
    }
   ],
   "source": [
    "!head mrthinks_0909.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'scallops')\n",
      "(1, 'onion_rings')\n",
      "(2, 'chicken_curry')\n",
      "(3, 'french_onion_soup')\n",
      "(4, 'dumplings')\n",
      "(5, 'nachos')\n",
      "(6, 'tuna_tartare')\n",
      "(7, 'gnocchi')\n",
      "(8, 'gyoza')\n",
      "(9, 'bruschetta')\n",
      "(10, 'croque_madame')\n",
      "(11, 'edamame')\n",
      "(12, 'grilled_salmon')\n",
      "(13, 'sashimi')\n",
      "(14, 'spring_rolls')\n",
      "(15, 'risotto')\n",
      "(16, 'prime_rib')\n",
      "(17, 'pajeon')\n",
      "(18, 'cannoli')\n",
      "(19, 'soondae')\n",
      "(20, 'doenjang_chigae')\n",
      "(21, 'waffles')\n",
      "(22, 'macarons')\n",
      "(23, 'foie_gras')\n",
      "(24, 'dakbokkeumtang')\n",
      "(25, 'ceviche')\n",
      "(26, 'cheese_plate')\n",
      "(27, 'hot_dog')\n",
      "(28, 'ojingeo_bokkeum')\n",
      "(29, 'creme_brulee')\n",
      "(30, 'steak')\n",
      "(31, 'falafel')\n",
      "(32, 'lasagna')\n",
      "(33, 'cup_cakes')\n",
      "(34, 'spaghetti_bolognese')\n",
      "(35, 'omelette')\n",
      "(36, 'filet_mignon')\n",
      "(37, 'soondubu_jjigae')\n",
      "(38, 'chocolate_mousse')\n",
      "(39, 'caesar_salad')\n",
      "(40, 'spaghetti_carbonara')\n",
      "(41, 'peking_duck')\n",
      "(42, 'macaroni_and_cheese')\n",
      "(43, 'pulled_pork_sandwich')\n",
      "(44, 'escargots')\n",
      "(45, 'chicken_wings')\n",
      "(46, 'takoyaki')\n",
      "(47, 'bulgogi')\n",
      "(48, 'donuts')\n",
      "(49, 'samosa')\n",
      "(50, 'miso_soup')\n",
      "(51, 'panna_cotta')\n",
      "(52, 'fish_and_chips')\n",
      "(53, 'tiramisu')\n",
      "(54, 'guacamole')\n",
      "(55, 'lobster_roll_sandwich')\n",
      "(56, 'pad_thai')\n",
      "(57, 'bibimbap')\n",
      "(58, 'deviled_eggs')\n",
      "(59, 'churros')\n",
      "(60, 'galchijorim')\n",
      "(61, 'baby_back_ribs')\n",
      "(62, 'shrimp_and_grits')\n",
      "(63, 'red_velvet_cake')\n",
      "(64, 'hamburger')\n",
      "(65, 'mussels')\n",
      "(66, 'samgyetang')\n",
      "(67, 'sushi')\n",
      "(68, 'tacos')\n",
      "(69, 'greek_salad')\n",
      "(70, 'garlic_bread')\n",
      "(71, 'fried_rice')\n",
      "(72, 'carrot_cake')\n",
      "(73, 'pancakes')\n",
      "(74, 'huevos_rancheros')\n",
      "(75, 'nangmyeon')\n",
      "(76, 'jjajangmyeon')\n",
      "(77, 'jeyuk_bokkeum')\n",
      "(78, 'chicken_quesadilla')\n",
      "(79, 'beef_carpaccio')\n",
      "(80, 'daegaejjim')\n",
      "(81, 'galbijjim')\n",
      "(82, 'lobster_bisque')\n",
      "(83, 'grilled_cheese_sandwich')\n",
      "(84, 'clam_chowder')\n",
      "(85, 'gimbob')\n",
      "(86, 'beet_salad')\n",
      "(87, 'eggs_benedict')\n",
      "(88, 'kimchi')\n",
      "(89, 'beignets')\n",
      "(90, 'breakfast_burrito')\n",
      "(91, 'french_fries')\n",
      "(92, 'french_toast')\n",
      "(93, 'hot_and_sour_soup')\n",
      "(94, 'seaweed_salad')\n",
      "(95, 'ice_cream')\n",
      "(96, 'pizza')\n",
      "(97, 'hummus')\n",
      "(98, 'fried_calamari')\n",
      "(99, 'oysters')\n",
      "(100, 'caprese_salad')\n",
      "(101, 'beef_tartare')\n",
      "(102, 'bossam')\n",
      "(103, 'pho')\n",
      "(104, 'bread_pudding')\n",
      "(105, 'club_sandwich')\n",
      "(106, 'apple_pie')\n",
      "(107, 'ganjang_gejang')\n",
      "(108, 'cheesecake')\n",
      "(109, 'poutine')\n",
      "(110, 'chocolate_cake')\n",
      "(111, 'frozen_yogurt')\n",
      "(112, 'ravioli')\n",
      "(113, 'samgupsal')\n",
      "(114, 'pork_chop')\n",
      "(115, 'crab_cakes')\n",
      "(116, 'ramen')\n",
      "(117, 'paella')\n",
      "(118, 'baklava')\n",
      "(119, 'strawberry_shortcake')\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(label_list):\n",
    "    print(i, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
